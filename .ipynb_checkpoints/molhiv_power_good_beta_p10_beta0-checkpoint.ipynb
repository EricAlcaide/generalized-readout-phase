{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "OBxXkQOYvhNz",
    "outputId": "43f1acc3-0082-4f3e-d923-7788da74b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.21.3)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.25.10)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.1.0)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (4.48.2)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.6.0+cu101)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.19.1)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (0.16.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: future in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from torch>=1.2.0->ogb) (0.18.2)\n",
      "Requirement already satisfied: littleutils in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6oAevLez2il"
   },
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_softmax, scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13A6KgTVui6Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_scatter import scatter, scatter_mean, scatter_add, scatter_sum\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "\n",
    "class AtomEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(9):\n",
    "            self.embeddings.append(Embedding(100, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(x.size(1)):\n",
    "            out += self.embeddings[i](x[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(3):\n",
    "            self.embeddings.append(Embedding(6, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        if edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(edge_attr.size(1)):\n",
    "            out += self.embeddings[i](edge_attr[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class Global_Gen_Sum_Mean_Max_Pool(nn.Module):\n",
    "    def __init__(self, family = \"softmax\", p = 1.0, beta = 1.0, \n",
    "                 trainable_p = False, trainable_beta = False):\n",
    "        r\"\"\"Performs batch-wise graph-level-outputs by transforming node\n",
    "        features based on a Generalized Aggr-Mean-Max function, so that\n",
    "        for a single graph :math:`\\mathcal{G}_i` its output is computed\n",
    "        deppending on the family of transformations by:\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{softmax} \\left( \\mathbf{x}_n * p \\right) * \\mathbf{x}_n\n",
    "        for softmax aggregation or\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\left( \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{x}_n^{p} \\right)^{1/p}\n",
    "        for power mean aggregation.\n",
    "\n",
    "        Args:\n",
    "            family (str): family of generalized mean-max functions to use. \n",
    "                Either \"softmax\" or \"power\" for eq. 1 or eq. 2 respectively.\n",
    "            p (float): parameter for the generalized mean-max function\n",
    "            trainable (bool): whether the value of p is learnable during training.\n",
    "        \"\"\"\n",
    "        super(Global_Gen_Sum_Mean_Max_Pool, self).__init__()\n",
    "        \n",
    "        self.family         = family\n",
    "        self.base_p         = p\n",
    "        self.base_beta      = beta\n",
    "        self.trainable_p    = trainable_p\n",
    "        self.trainable_beta = trainable_beta\n",
    "        # define params\n",
    "        self.p = torch.nn.Parameter(torch.tensor([p], device=device),\n",
    "                                    requires_grad=trainable_p)# .to(device)\n",
    "        self.beta = torch.nn.Parameter(torch.tensor([beta], device=device),\n",
    "                                       requires_grad=trainable_beta)# .to(device)\n",
    "\n",
    "    def forward(self, x, batch, bsize=None):\n",
    "        r\"\"\"Args:\n",
    "            x (Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "            batch (LongTensor): Batch vector :math:`\\mathbf{b} \\in {\\{ 0, \\ldots,\n",
    "                B-1\\}}^N`, which assigns each node to a specific example.\n",
    "            size (int, optional): Batch-size :math:`B`.\n",
    "                Automatically calculated if not given. (default: :obj:`None`)\n",
    "        :rtype: :class:`Tensor`\n",
    "        \"\"\"\n",
    "        bsize = int(batch.max().item() + 1) if bsize is None else bsize\n",
    "        n_nodes = scatter_sum(torch.ones_like(x), batch, dim=0, dim_size=bsize)\n",
    "        if self.family == \"softmax\":\n",
    "            out = scatter_softmax(self.p * x.detach(), batch, dim=0)\n",
    "            return scatter_add(x * out,\n",
    "                                batch, dim=0, dim_size=bsize)*n_nodes / (1+self.beta*(n_nodes-1))\n",
    "\n",
    "        elif self.family == \"power\":\n",
    "            #Â numerical stability - avoid powers of large numbers or negative ones\n",
    "            min_x, max_x = 1e-7, 1e+3\n",
    "            torch.clamp_(x, min_x, max_x)\n",
    "            out = scatter_add(torch.pow(x, self.p),\n",
    "                               batch, dim=0, dim_size=bsize) / (1+self.beta*(n_nodes-1))\n",
    "            torch.clamp_(out, min_x, max_x)\n",
    "            return torch.pow(out, 1 / self.p)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.p and torch.is_tensor(self.p):\n",
    "            self.p.data.fill_(self.base_p)\n",
    "        if self.beta and torch.is_tensor(self.beta):\n",
    "            self.beta.data.fill_(self.base_beta)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Generalized Aggr-Mean-Max global pooling layer with params:\" + \\\n",
    "               str({\"family\": self.family,\n",
    "                    \"base_p\": self.base_p,\n",
    "                    \"base_beta\"     : self.base_beta,\n",
    "                    \"trainable_p\"   : self.trainable_p,\n",
    "                    \"trainable_beta\": self.trainable_beta})\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=0.0,\n",
    "                 inter_message_passing=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.inter_message_passing = inter_message_passing\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(hidden_channels)\n",
    "        self.clique_encoder = Embedding(4, hidden_channels)\n",
    "\n",
    "        self.bond_encoders = ModuleList()\n",
    "        self.atom_convs = ModuleList()\n",
    "        self.atom_batch_norms = ModuleList()\n",
    "        self.reader = Global_Gen_Sum_Mean_Max_Pool(family = \"power\", p = 10.0, beta = 1e-5, \n",
    "                                                   trainable_p = True, trainable_beta = True)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.bond_encoders.append(BondEncoder(hidden_channels))\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.atom_convs.append(GINEConv(nn, train_eps=True))\n",
    "            self.atom_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.clique_convs = ModuleList()\n",
    "        self.clique_batch_norms = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.clique_convs.append(GINConv(nn, train_eps=True))\n",
    "            self.clique_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.atom2clique_lins = ModuleList()\n",
    "        self.clique2atom_lins = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.atom2clique_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "            self.clique2atom_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.atom_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.clique_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.atom_encoder.reset_parameters()\n",
    "        self.clique_encoder.reset_parameters()\n",
    "\n",
    "        for emb, conv, batch_norm in zip(self.bond_encoders, self.atom_convs,\n",
    "                                         self.atom_batch_norms):\n",
    "            emb.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for conv, batch_norm in zip(self.clique_convs,\n",
    "                                    self.clique_batch_norms):\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for lin1, lin2 in zip(self.atom2clique_lins, self.clique2atom_lins):\n",
    "            lin1.reset_parameters()\n",
    "            lin2.reset_parameters()\n",
    "\n",
    "        self.atom_lin.reset_parameters()\n",
    "        self.clique_lin.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        self.reader.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.squeeze())\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            x_clique = self.clique_encoder(data.x_clique.squeeze())\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            edge_attr = self.bond_encoders[i](data.edge_attr)\n",
    "            x = self.atom_convs[i](x, data.edge_index, edge_attr)\n",
    "            x = self.atom_batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "            if self.inter_message_passing:\n",
    "                row, col = data.atom2clique_index\n",
    "\n",
    "                x_clique = x_clique + F.relu(self.atom2clique_lins[i](scatter(\n",
    "                    x[row], col, dim=0, dim_size=x_clique.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "                x_clique = self.clique_convs[i](x_clique, data.tree_edge_index)\n",
    "                x_clique = self.clique_batch_norms[i](x_clique)\n",
    "                x_clique = F.relu(x_clique)\n",
    "                x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                     training=self.training)\n",
    "\n",
    "                x = x + F.relu(self.clique2atom_lins[i](scatter(\n",
    "                    x_clique[col], row, dim=0, dim_size=x.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "        x = self.reader(x, data.batch)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.atom_lin(x)\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            tree_batch = torch.repeat_interleave(data.num_cliques)\n",
    "            x_clique = scatter(x_clique, tree_batch, dim=0, dim_size=x.size(0),\n",
    "                               reduce='mean')\n",
    "            x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                 training=self.training)\n",
    "            x_clique = self.clique_lin(x_clique)\n",
    "            x = x + x_clique\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkP00CdeukM5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import tree_decomposition\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "\n",
    "bonds = [BondType.SINGLE, BondType.DOUBLE, BondType.TRIPLE, BondType.AROMATIC]\n",
    "\n",
    "\n",
    "def mol_from_data(data):\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    x = data.x if data.x.dim() == 1 else data.x[:, 0]\n",
    "    for z in x.tolist():\n",
    "        mol.AddAtom(Chem.Atom(z))\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask].tolist(), col[mask].tolist()\n",
    "\n",
    "    bond_type = data.edge_attr\n",
    "    bond_type = bond_type if bond_type.dim() == 1 else bond_type[:, 0]\n",
    "    bond_type = bond_type[mask].tolist()\n",
    "\n",
    "    for i, j, bond in zip(row, col, bond_type):\n",
    "        assert bond >= 1 and bond <= 4\n",
    "        mol.AddBond(i, j, bonds[bond - 1])\n",
    "\n",
    "    return mol.GetMol()\n",
    "\n",
    "\n",
    "class JunctionTreeData(Data):\n",
    "    def __inc__(self, key, item):\n",
    "        if key == 'tree_edge_index':\n",
    "            return self.x_clique.size(0)\n",
    "        elif key == 'atom2clique_index':\n",
    "            return torch.tensor([[self.x.size(0)], [self.x_clique.size(0)]])\n",
    "        else:\n",
    "            return super(JunctionTreeData, self).__inc__(key, item)\n",
    "\n",
    "\n",
    "class JunctionTree(object):\n",
    "    def __call__(self, data):\n",
    "        mol = mol_from_data(data)\n",
    "        out = tree_decomposition(mol, return_vocab=True)\n",
    "        tree_edge_index, atom2clique_index, num_cliques, x_clique = out\n",
    "\n",
    "        data = JunctionTreeData(**{k: v for k, v in data})\n",
    "\n",
    "        data.tree_edge_index = tree_edge_index\n",
    "        data.atom2clique_index = atom2clique_index\n",
    "        data.num_cliques = num_cliques\n",
    "        data.x_clique = x_clique\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-BnRJKkukQ8"
   },
   "outputs": [],
   "source": [
    "# edit the function causing the error: add argument chem=None + modify function code: \n",
    "# Chem=chem if chem is not None else Chem\n",
    "\n",
    "# tree_decomposition(Chem.MolFromSmiles(\"cicccc1c\"), return_vocab=True)\n",
    "#Â once modified and saved, restart the environmnet, comment this cell and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "l-eGz7HtCLCr",
    "outputId": "a5812a61-fea7-4301-dd16-9d75ea62035b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "class Argparse_emulate():\n",
    "    def __init__(self, device=0, hidden_channels=256, num_layers=2, dropout=0.5,\n",
    "               epochs=100, no_inter_message_passing=\"store_true\"):\n",
    "        self.device = device\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.no_inter_message_passing = no_inter_message_passing\n",
    "        return\n",
    "\n",
    "args = Argparse_emulate()\n",
    "# parser.add_argument('--device', type=int, default=0)\n",
    "# parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "# parser.add_argument('--num_layers', type=int, default=2)\n",
    "# parser.add_argument('--dropout', type=float, default=0.5)\n",
    "# parser.add_argument('--epochs', type=int, default=100)\n",
    "# parser.add_argument('--no_inter_message_passing', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "\n",
    "class OGBTransform(object):\n",
    "    # OGB saves atom and bond types zero-index based. We need to revert that.\n",
    "    def __call__(self, data):\n",
    "        data.x[:, 0] += 1\n",
    "        data.edge_attr[:, 0] += 1\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = Compose([OGBTransform(), JunctionTree()])\n",
    "\n",
    "name = 'ogbg-molhiv'\n",
    "dataset = PygGraphPropPredDataset(name, 'data', pre_transform=transform)\n",
    "\n",
    "# correct splits\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]\n",
    "val_dataset = dataset[split_idx['valid']]\n",
    "test_dataset = dataset[split_idx['test']]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)\n",
    "\n",
    "\n",
    "device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, vals=False):\n",
    "    values = []\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = ~torch.isnan(data.y)\n",
    "        out = model(data)[mask]\n",
    "        y = data.y.to(torch.float)[mask]\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(out, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        #Â record beta and p values\n",
    "        if vals:\n",
    "            # display computational graph\n",
    "            # global g\n",
    "            #g = make_dot(out)\n",
    "            #Â \"a\"+9\n",
    "            values_batch = {}\n",
    "            values_batch[\"p\"]    = model.reader.p.detach().cpu().numpy()\n",
    "            values_batch[\"beta\"] = model.reader.beta.detach().cpu().numpy()\n",
    "            values.append(values_batch)\n",
    "            if False: # i==0:\n",
    "                print(\"records:\", i, \"value:\", values_batch)\n",
    "\n",
    "\n",
    "    return total_loss / len(train_loader.dataset), values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_preds, y_trues = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        y_preds.append(model(data))\n",
    "        y_trues.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(y_preds, dim=0).cpu().numpy()\n",
    "    y_true = torch.cat(y_trues, dim=0).cpu().numpy()\n",
    "\n",
    "    rocauc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        # AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
    "            # ignore nan values\n",
    "            is_labeled = y_true[:, i] == y_true[:, i]\n",
    "            rocauc_list.append(\n",
    "                roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i]))\n",
    "\n",
    "    return {\"rocauc\": sum(rocauc_list) / len(rocauc_list)}\n",
    "\n",
    "\n",
    "values     = []\n",
    "test_perfs = []\n",
    "for run in range(10):\n",
    "    print()\n",
    "    print(f'Run {run}:')\n",
    "    print()\n",
    "    model = Net(hidden_channels=args.hidden_channels,\n",
    "            out_channels=dataset.num_tasks, num_layers=args.num_layers,\n",
    "            dropout = args.dropout if run<10 else 0.6, # edited to increase dropout\n",
    "            inter_message_passing=not args.no_inter_message_passing).to(device)\n",
    "\n",
    "    model.reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    best_val_perf = test_perf = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss, epoch_values = train(epoch, vals=True)\n",
    "        train_perf = test(train_loader)\n",
    "        val_perf = test(val_loader)\n",
    "\n",
    "        if val_perf[\"rocauc\"] > best_val_perf:\n",
    "            best_val_perf = val_perf[\"rocauc\"]\n",
    "            test_perf = test(test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_perf[\"rocauc\"]:.4f}, Val: {val_perf[\"rocauc\"]:.4f}, '\n",
    "              f'Test: {test_perf[\"rocauc\"]:.4f}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Recorded values:\", epoch_values)\n",
    "\n",
    "    test_perfs.append(test_perf[\"rocauc\"])\n",
    "    values.append(epoch_values[-1])\n",
    "\n",
    "test_perf = torch.tensor(test_perfs)\n",
    "print('===========================')\n",
    "print(f'Final Test: {test_perf.mean():.4f} Â± {test_perf.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize param evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4h68JnX3CLJo"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "p_epoch    = np.array([batch[\"p\"] for batch in epoch_values]).flatten()\n",
    "beta_epoch = np.array([batch[\"beta\"] for batch in epoch_values]).flatten() \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Evolution of parameters beta and p during training - Power Mean\")\n",
    "plt.plot(p_epoch, \"r-\", label=\"p\")\n",
    "plt.plot(beta_epoch, \"b-\", label=\"beta\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMZ16-Q3CLMD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9RNK8apbCLOX"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de analysis_molhiv_power.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
