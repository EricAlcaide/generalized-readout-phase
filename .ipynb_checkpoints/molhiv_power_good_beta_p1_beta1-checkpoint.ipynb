{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "OBxXkQOYvhNz",
    "outputId": "43f1acc3-0082-4f3e-d923-7788da74b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.21.3)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.6.0+cu101)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.25.10)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.15.0)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (4.48.2)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (0.16.0)\n",
      "Requirement already satisfied: future in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from torch>=1.2.0->ogb) (0.18.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.24.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2020.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6oAevLez2il"
   },
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_softmax, scatter_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13A6KgTVui6Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_scatter import scatter, scatter_mean, scatter_add, scatter_sum\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "\n",
    "class AtomEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(9):\n",
    "            self.embeddings.append(Embedding(100, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(x.size(1)):\n",
    "            out += self.embeddings[i](x[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(3):\n",
    "            self.embeddings.append(Embedding(6, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        if edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(edge_attr.size(1)):\n",
    "            out += self.embeddings[i](edge_attr[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class Global_Gen_Sum_Mean_Max_Pool(nn.Module):\n",
    "    def __init__(self, family = \"softmax\", p = 1.0, beta = 1.0, \n",
    "                 trainable_p = False, trainable_beta = False):\n",
    "        r\"\"\"Performs batch-wise graph-level-outputs by transforming node\n",
    "        features based on a Generalized Aggr-Mean-Max function, so that\n",
    "        for a single graph :math:`\\mathcal{G}_i` its output is computed\n",
    "        deppending on the family of transformations by:\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{softmax} \\left( \\mathbf{x}_n * p \\right) * \\mathbf{x}_n\n",
    "        for softmax aggregation or\n",
    "        .. math::\n",
    "            \\mathbf{r}_i = \\left( \\frac{1}{\\beta*N_i} \\sum_{n=1}^{N_i} \\mathbf{x}_n^{p} \\right)^{1/p}\n",
    "        for power mean aggregation.\n",
    "\n",
    "        Args:\n",
    "            family (str): family of generalized mean-max functions to use. \n",
    "                Either \"softmax\" or \"power\" for eq. 1 or eq. 2 respectively.\n",
    "            p (float): parameter for the generalized mean-max function\n",
    "            trainable (bool): whether the value of p is learnable during training.\n",
    "        \"\"\"\n",
    "        super(Global_Gen_Sum_Mean_Max_Pool, self).__init__()\n",
    "        \n",
    "        self.family         = family\n",
    "        self.base_p         = p\n",
    "        self.base_beta      = beta\n",
    "        self.trainable_p    = trainable_p\n",
    "        self.trainable_beta = trainable_beta\n",
    "        # define params\n",
    "        self.p = torch.nn.Parameter(torch.tensor([p], device=device),\n",
    "                                    requires_grad=trainable_p)# .to(device)\n",
    "        self.beta = torch.nn.Parameter(torch.tensor([beta], device=device),\n",
    "                                       requires_grad=trainable_beta)# .to(device)\n",
    "\n",
    "    def forward(self, x, batch, bsize=None):\n",
    "        r\"\"\"Args:\n",
    "            x (Tensor): Node feature matrix\n",
    "                :math:`\\mathbf{X} \\in \\mathbb{R}^{(N_1 + \\ldots + N_B) \\times F}`.\n",
    "            batch (LongTensor): Batch vector :math:`\\mathbf{b} \\in {\\{ 0, \\ldots,\n",
    "                B-1\\}}^N`, which assigns each node to a specific example.\n",
    "            size (int, optional): Batch-size :math:`B`.\n",
    "                Automatically calculated if not given. (default: :obj:`None`)\n",
    "        :rtype: :class:`Tensor`\n",
    "        \"\"\"\n",
    "        bsize = int(batch.max().item() + 1) if bsize is None else bsize\n",
    "        n_nodes = scatter_sum(torch.ones_like(x), batch, dim=0, dim_size=bsize)\n",
    "        if self.family == \"softmax\":\n",
    "            out = scatter_softmax(self.p * x.detach(), batch, dim=0)\n",
    "            return scatter_add(x * out,\n",
    "                                batch, dim=0, dim_size=bsize)*n_nodes / (1+self.beta*(n_nodes-1))\n",
    "\n",
    "        elif self.family == \"power\":\n",
    "            #Â numerical stability - avoid powers of large numbers or negative ones\n",
    "            min_x, max_x = 1e-7, 1e+3\n",
    "            torch.clamp_(x, min_x, max_x)\n",
    "            out = scatter_add(torch.pow(x, self.p),\n",
    "                               batch, dim=0, dim_size=bsize) / (1+self.beta*(n_nodes-1))\n",
    "            torch.clamp_(out, min_x, max_x)\n",
    "            return torch.pow(out, 1 / self.p)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.p and torch.is_tensor(self.p):\n",
    "            self.p.data.fill_(self.base_p)\n",
    "        if self.beta and torch.is_tensor(self.beta):\n",
    "            self.beta.data.fill_(self.base_beta)\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Generalized Aggr-Mean-Max global pooling layer with params:\" + \\\n",
    "               str({\"family\": self.family,\n",
    "                    \"base_p\": self.base_p,\n",
    "                    \"base_beta\"     : self.base_beta,\n",
    "                    \"trainable_p\"   : self.trainable_p,\n",
    "                    \"trainable_beta\": self.trainable_beta})\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=0.0,\n",
    "                 inter_message_passing=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.inter_message_passing = inter_message_passing\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(hidden_channels)\n",
    "        self.clique_encoder = Embedding(4, hidden_channels)\n",
    "\n",
    "        self.bond_encoders = ModuleList()\n",
    "        self.atom_convs = ModuleList()\n",
    "        self.atom_batch_norms = ModuleList()\n",
    "        self.reader = Global_Gen_Sum_Mean_Max_Pool(family = \"power\", p = 1.0, beta = 1.0, \n",
    "                                                   trainable_p = True, trainable_beta = True)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.bond_encoders.append(BondEncoder(hidden_channels))\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.atom_convs.append(GINEConv(nn, train_eps=True))\n",
    "            self.atom_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.clique_convs = ModuleList()\n",
    "        self.clique_batch_norms = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.clique_convs.append(GINConv(nn, train_eps=True))\n",
    "            self.clique_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.atom2clique_lins = ModuleList()\n",
    "        self.clique2atom_lins = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.atom2clique_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "            self.clique2atom_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.atom_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.clique_lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.atom_encoder.reset_parameters()\n",
    "        self.clique_encoder.reset_parameters()\n",
    "\n",
    "        for emb, conv, batch_norm in zip(self.bond_encoders, self.atom_convs,\n",
    "                                         self.atom_batch_norms):\n",
    "            emb.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for conv, batch_norm in zip(self.clique_convs,\n",
    "                                    self.clique_batch_norms):\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for lin1, lin2 in zip(self.atom2clique_lins, self.clique2atom_lins):\n",
    "            lin1.reset_parameters()\n",
    "            lin2.reset_parameters()\n",
    "\n",
    "        self.atom_lin.reset_parameters()\n",
    "        self.clique_lin.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        self.reader.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.squeeze())\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            x_clique = self.clique_encoder(data.x_clique.squeeze())\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            edge_attr = self.bond_encoders[i](data.edge_attr)\n",
    "            x = self.atom_convs[i](x, data.edge_index, edge_attr)\n",
    "            x = self.atom_batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "            if self.inter_message_passing:\n",
    "                row, col = data.atom2clique_index\n",
    "\n",
    "                x_clique = x_clique + F.relu(self.atom2clique_lins[i](scatter(\n",
    "                    x[row], col, dim=0, dim_size=x_clique.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "                x_clique = self.clique_convs[i](x_clique, data.tree_edge_index)\n",
    "                x_clique = self.clique_batch_norms[i](x_clique)\n",
    "                x_clique = F.relu(x_clique)\n",
    "                x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                     training=self.training)\n",
    "\n",
    "                x = x + F.relu(self.clique2atom_lins[i](scatter(\n",
    "                    x_clique[col], row, dim=0, dim_size=x.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "        x = self.reader(x, data.batch)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.atom_lin(x)\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            tree_batch = torch.repeat_interleave(data.num_cliques)\n",
    "            x_clique = scatter(x_clique, tree_batch, dim=0, dim_size=x.size(0),\n",
    "                               reduce='mean')\n",
    "            x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                 training=self.training)\n",
    "            x_clique = self.clique_lin(x_clique)\n",
    "            x = x + x_clique\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkP00CdeukM5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import tree_decomposition\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "\n",
    "bonds = [BondType.SINGLE, BondType.DOUBLE, BondType.TRIPLE, BondType.AROMATIC]\n",
    "\n",
    "\n",
    "def mol_from_data(data):\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    x = data.x if data.x.dim() == 1 else data.x[:, 0]\n",
    "    for z in x.tolist():\n",
    "        mol.AddAtom(Chem.Atom(z))\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask].tolist(), col[mask].tolist()\n",
    "\n",
    "    bond_type = data.edge_attr\n",
    "    bond_type = bond_type if bond_type.dim() == 1 else bond_type[:, 0]\n",
    "    bond_type = bond_type[mask].tolist()\n",
    "\n",
    "    for i, j, bond in zip(row, col, bond_type):\n",
    "        assert bond >= 1 and bond <= 4\n",
    "        mol.AddBond(i, j, bonds[bond - 1])\n",
    "\n",
    "    return mol.GetMol()\n",
    "\n",
    "\n",
    "class JunctionTreeData(Data):\n",
    "    def __inc__(self, key, item):\n",
    "        if key == 'tree_edge_index':\n",
    "            return self.x_clique.size(0)\n",
    "        elif key == 'atom2clique_index':\n",
    "            return torch.tensor([[self.x.size(0)], [self.x_clique.size(0)]])\n",
    "        else:\n",
    "            return super(JunctionTreeData, self).__inc__(key, item)\n",
    "\n",
    "\n",
    "class JunctionTree(object):\n",
    "    def __call__(self, data):\n",
    "        mol = mol_from_data(data)\n",
    "        out = tree_decomposition(mol, return_vocab=True)\n",
    "        tree_edge_index, atom2clique_index, num_cliques, x_clique = out\n",
    "\n",
    "        data = JunctionTreeData(**{k: v for k, v in data})\n",
    "\n",
    "        data.tree_edge_index = tree_edge_index\n",
    "        data.atom2clique_index = atom2clique_index\n",
    "        data.num_cliques = num_cliques\n",
    "        data.x_clique = x_clique\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-BnRJKkukQ8"
   },
   "outputs": [],
   "source": [
    "# edit the function causing the error: add argument chem=None + modify function code: \n",
    "# Chem=chem if chem is not None else Chem\n",
    "\n",
    "# tree_decomposition(Chem.MolFromSmiles(\"cicccc1c\"), return_vocab=True)\n",
    "#Â once modified and saved, restart the environmnet, comment this cell and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "l-eGz7HtCLCr",
    "outputId": "a5812a61-fea7-4301-dd16-9d75ea62035b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "class Argparse_emulate():\n",
    "    def __init__(self, device=0, hidden_channels=256, num_layers=2, dropout=0.5,\n",
    "               epochs=100, no_inter_message_passing=\"store_true\"):\n",
    "        self.device = device\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.no_inter_message_passing = no_inter_message_passing\n",
    "        return\n",
    "\n",
    "args = Argparse_emulate()\n",
    "# parser.add_argument('--device', type=int, default=0)\n",
    "# parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "# parser.add_argument('--num_layers', type=int, default=2)\n",
    "# parser.add_argument('--dropout', type=float, default=0.5)\n",
    "# parser.add_argument('--epochs', type=int, default=100)\n",
    "# parser.add_argument('--no_inter_message_passing', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "\n",
    "class OGBTransform(object):\n",
    "    # OGB saves atom and bond types zero-index based. We need to revert that.\n",
    "    def __call__(self, data):\n",
    "        data.x[:, 0] += 1\n",
    "        data.edge_attr[:, 0] += 1\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = Compose([OGBTransform(), JunctionTree()])\n",
    "\n",
    "name = 'ogbg-molhiv'\n",
    "dataset = PygGraphPropPredDataset(name, 'data', pre_transform=transform)\n",
    "\n",
    "# correct splits\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]\n",
    "val_dataset = dataset[split_idx['valid']]\n",
    "test_dataset = dataset[split_idx['test']]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)\n",
    "\n",
    "\n",
    "device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(epoch, vals=False):\n",
    "    values = []\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i,data in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = ~torch.isnan(data.y)\n",
    "        out = model(data)[mask]\n",
    "        y = data.y.to(torch.float)[mask]\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(out, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        #Â record beta and p values\n",
    "        if vals:\n",
    "            # display computational graph\n",
    "            # global g\n",
    "            #g = make_dot(out)\n",
    "            #Â \"a\"+9\n",
    "            values_batch = {}\n",
    "            values_batch[\"p\"]    = model.reader.p.detach().cpu().numpy()\n",
    "            values_batch[\"beta\"] = model.reader.beta.detach().cpu().numpy()\n",
    "            values.append(values_batch)\n",
    "            if False: # i==0:\n",
    "                print(\"records:\", i, \"value:\", values_batch)\n",
    "\n",
    "\n",
    "    return total_loss / len(train_loader.dataset), values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_preds, y_trues = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        y_preds.append(model(data))\n",
    "        y_trues.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(y_preds, dim=0).cpu().numpy()\n",
    "    y_true = torch.cat(y_trues, dim=0).cpu().numpy()\n",
    "\n",
    "    rocauc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        # AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
    "            # ignore nan values\n",
    "            is_labeled = y_true[:, i] == y_true[:, i]\n",
    "            rocauc_list.append(\n",
    "                roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i]))\n",
    "\n",
    "    return {\"rocauc\": sum(rocauc_list) / len(rocauc_list)}\n",
    "\n",
    "\n",
    "values     = []\n",
    "test_perfs = []\n",
    "for run in range(10):\n",
    "    print()\n",
    "    print(f'Run {run}:')\n",
    "    print()\n",
    "    model = Net(hidden_channels=args.hidden_channels,\n",
    "            out_channels=dataset.num_tasks, num_layers=args.num_layers,\n",
    "            dropout = args.dropout if run<10 else 0.6, # edited to increase dropout\n",
    "            inter_message_passing=not args.no_inter_message_passing).to(device)\n",
    "\n",
    "    model.reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    best_val_perf = test_perf = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss, epoch_values = train(epoch, vals=True)\n",
    "        train_perf = test(train_loader)\n",
    "        val_perf = test(val_loader)\n",
    "\n",
    "        if val_perf[\"rocauc\"] > best_val_perf:\n",
    "            best_val_perf = val_perf[\"rocauc\"]\n",
    "            test_perf = test(test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_perf[\"rocauc\"]:.4f}, Val: {val_perf[\"rocauc\"]:.4f}, '\n",
    "              f'Test: {test_perf[\"rocauc\"]:.4f}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Recorded values:\", epoch_values)\n",
    "\n",
    "    test_perfs.append(test_perf[\"rocauc\"])\n",
    "    values.append(epoch_values[-1])\n",
    "\n",
    "test_perf = torch.tensor(test_perfs)\n",
    "print('===========================')\n",
    "print(f'Final Test: {test_perf.mean():.4f} Â± {test_perf.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_epoch    = np.array([batch[\"p\"] for batch in epoch_values]).flatten()\n",
    "beta_epoch = np.array([batch[\"beta\"] for batch in epoch_values]).flatten() \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Evolution of parameters beta and p during training - Power Mean\")\n",
    "plt.plot(p_epoch, \"r-\", label=\"p\")\n",
    "plt.plot(beta_epoch, \"b-\", label=\"beta\")\n",
    "plt.ylim(0, 1.1*np.amax(np.maximum(p_epoch, beta_epoch)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de analysis_molhiv_power.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
