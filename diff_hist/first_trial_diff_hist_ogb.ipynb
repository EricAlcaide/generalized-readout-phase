{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "OBxXkQOYvhNz",
    "outputId": "43f1acc3-0082-4f3e-d923-7788da74b19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ogb in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (1.2.3)\n",
      "Requirement already satisfied: torch>=1.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.6.0+cu101)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.19.1)\n",
      "Requirement already satisfied: tqdm>=4.29.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (4.48.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.15.0)\n",
      "Requirement already satisfied: outdated>=0.2.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (0.2.0)\n",
      "Requirement already satisfied: pandas>=0.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.1.0)\n",
      "Requirement already satisfied: urllib3>=1.24.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from ogb) (1.25.10)\n",
      "Requirement already satisfied: future in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from torch>=1.2.0->ogb) (0.18.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (0.16.0)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from scikit-learn>=0.20.0->ogb) (1.5.0)\n",
      "Requirement already satisfied: littleutils in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (0.2.2)\n",
      "Requirement already satisfied: requests in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from outdated>=0.2.0->ogb) (2.24.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2020.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from pandas>=0.24.0->ogb) (2.8.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\eric\\miniconda3\\envs\\my-rdkit-env-tf1\\lib\\site-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install ogb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R6oAevLez2il"
   },
   "outputs": [],
   "source": [
    "from torch_scatter import scatter_softmax, scatter_mean\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "13A6KgTVui6Q"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Embedding, ModuleList\n",
    "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\n",
    "from torch_scatter import scatter, scatter_mean, scatter_add, scatter_sum\n",
    "from torch_geometric.nn import GINConv, GINEConv\n",
    "\n",
    "\n",
    "class AtomEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(AtomEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(9):\n",
    "            self.embeddings.append(Embedding(100, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 1:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(x.size(1)):\n",
    "            out += self.embeddings[i](x[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class BondEncoder(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(BondEncoder, self).__init__()\n",
    "\n",
    "        self.embeddings = torch.nn.ModuleList()\n",
    "\n",
    "        for i in range(3):\n",
    "            self.embeddings.append(Embedding(6, hidden_channels))\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        for embedding in self.embeddings:\n",
    "            embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, edge_attr):\n",
    "        if edge_attr.dim() == 1:\n",
    "            edge_attr = edge_attr.unsqueeze(1)\n",
    "\n",
    "        out = 0\n",
    "        for i in range(edge_attr.size(1)):\n",
    "            out += self.embeddings[i](edge_attr[:, i])\n",
    "        return out\n",
    "\n",
    "\n",
    "class Diff_Hist(nn.Module):\n",
    "    def __init__(self, centers=[-5, -2.5, 0, 2.5, 5], scale=2):\n",
    "        super(Diff_Hist, self).__init__()\n",
    "        \"\"\" centers: center of the bins used. \n",
    "            scale  : exponential scale for sigmoid. the higher,\n",
    "                the lower the amplitud of the bins.\n",
    "        \"\"\"\n",
    "        # save params\n",
    "        self.scale        = scale\n",
    "        self.true_centers = centers\n",
    "        self.max_scaler   = 1\n",
    "        self.max_scaler   = torch.max(self.func(torch.arange(start=-1, end=1, step=1e-3)))\n",
    "        # our function is centered @ 0.5 by default\n",
    "        self.centers      = 0.5 + np.array(self.true_centers)\n",
    "\n",
    "    def forward(self, x, batch=None, bsize=None):\n",
    "        r\"\"\" reduces dim=0 as a set of bins. designed for 2d outs. \"\"\"\n",
    "        out = torch.zeros(len(self.centers), *x.shape[1:], device=device)\n",
    "        for i, center in enumerate(self.centers):\n",
    "            out[i]  = self.make_bin(x, center)\n",
    "        return out / x.shape[0]\n",
    "\n",
    "    def make_bin(self, x, center):\n",
    "        return self.func(x+center).sum(dim=0, keepdim=True)\n",
    "        \n",
    "    def func(self, x):\n",
    "        # augmenting sigmoid so higher gradient and localized feats\n",
    "        sigmoid_x = lambda y: 1/(1+torch.exp(-self.scale*y))\n",
    "        # perfrom minmax scaling so max is 1 and min (is already) 0\n",
    "        return (sigmoid_x(x)-sigmoid_x(x-1))/self.max_scaler\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Simple differentiable histogram layer:\" + \\\n",
    "               str({\"centers\"   : self.true_centers,\n",
    "                    \"exp_scale\" : self.scale})\n",
    "    \n",
    "\n",
    "class Readout_Hist(nn.Module):\n",
    "    def __init__(self, centers):\n",
    "        super(Readout_Hist, self).__init__()\n",
    "        # save centers\n",
    "        self.true_centers = centers\n",
    "        self.diff_hist    = Diff_Hist(centers=centers)\n",
    "\n",
    "    def forward(self, x, batch=None, bsize=None, dim=0):\n",
    "        r\"\"\" reduces dim=0 as a set of bins. designed for 2d outs. \"\"\"\n",
    "        bsize = bsize if bsize is not None else int(torch.max(batch).cpu().numpy())+1\n",
    "        out   = torch.empty(bsize, len(self.true_centers), *x.shape[1:], device=device)\n",
    "        # for all unique nodes\n",
    "        for n in range(bsize):\n",
    "            # create a multi-center hist for their feats\n",
    "            out[n]  = self.diff_hist(x[batch==n])\n",
    "        return out.reshape(out.shape[0], -1)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return \"Readout by histogram:\" + str({\"centers\": self.true_centers})\n",
    "\n",
    "\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, num_layers, dropout=0.0,\n",
    "                 inter_message_passing=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.inter_message_passing = inter_message_passing\n",
    "\n",
    "        self.atom_encoder = AtomEncoder(hidden_channels)\n",
    "        self.clique_encoder = Embedding(4, hidden_channels)\n",
    "\n",
    "        self.bond_encoders = ModuleList()\n",
    "        self.atom_convs = ModuleList()\n",
    "        self.atom_batch_norms = ModuleList()\n",
    "        \n",
    "        # DIFFERENTIABLE HISTOGRAMS\n",
    "        offset, n   = 2, 5 # from -(2*5) to 2*5 \n",
    "        centers     = list(range(-offset*n, offset*n+1))\n",
    "        self.reader = Readout_Hist(centers=centers)\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.bond_encoders.append(BondEncoder(hidden_channels))\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.atom_convs.append(GINEConv(nn, train_eps=True))\n",
    "            self.atom_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.clique_convs = ModuleList()\n",
    "        self.clique_batch_norms = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            nn = Sequential(\n",
    "                Linear(hidden_channels, 2 * hidden_channels),\n",
    "                BatchNorm1d(2 * hidden_channels),\n",
    "                ReLU(),\n",
    "                Linear(2 * hidden_channels, hidden_channels),\n",
    "            )\n",
    "            self.clique_convs.append(GINConv(nn, train_eps=True))\n",
    "            self.clique_batch_norms.append(BatchNorm1d(hidden_channels))\n",
    "\n",
    "        self.atom2clique_lins = ModuleList()\n",
    "        self.clique2atom_lins = ModuleList()\n",
    "\n",
    "        for _ in range(num_layers):\n",
    "            self.atom2clique_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "            self.clique2atom_lins.append(\n",
    "                Linear(hidden_channels, hidden_channels))\n",
    "\n",
    "        self.atom_lin = Linear(hidden_channels*len(centers), hidden_channels)\n",
    "        self.clique_lin = Linear(hidden_channels, hidden_channels)\n",
    "        # final layer merges info from readout + clique\n",
    "        self.lin = Linear(hidden_channels, out_channels)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.atom_encoder.reset_parameters()\n",
    "        self.clique_encoder.reset_parameters()\n",
    "\n",
    "        for emb, conv, batch_norm in zip(self.bond_encoders, self.atom_convs,\n",
    "                                         self.atom_batch_norms):\n",
    "            emb.reset_parameters()\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for conv, batch_norm in zip(self.clique_convs,\n",
    "                                    self.clique_batch_norms):\n",
    "            conv.reset_parameters()\n",
    "            batch_norm.reset_parameters()\n",
    "\n",
    "        for lin1, lin2 in zip(self.atom2clique_lins, self.clique2atom_lins):\n",
    "            lin1.reset_parameters()\n",
    "            lin2.reset_parameters()\n",
    "\n",
    "        self.atom_lin.reset_parameters()\n",
    "        self.clique_lin.reset_parameters()\n",
    "        self.lin.reset_parameters()\n",
    "        # self.reader.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x = self.atom_encoder(data.x.squeeze())\n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            x_clique = self.clique_encoder(data.x_clique.squeeze())\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            edge_attr = self.bond_encoders[i](data.edge_attr)\n",
    "            x = self.atom_convs[i](x, data.edge_index, edge_attr)\n",
    "            x = self.atom_batch_norms[i](x)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, self.dropout, training=self.training)\n",
    "\n",
    "            if self.inter_message_passing:\n",
    "                row, col = data.atom2clique_index\n",
    "\n",
    "                x_clique = x_clique + F.relu(self.atom2clique_lins[i](scatter(\n",
    "                    x[row], col, dim=0, dim_size=x_clique.size(0),\n",
    "                    reduce='mean')))\n",
    "\n",
    "                x_clique = self.clique_convs[i](x_clique, data.tree_edge_index)\n",
    "                x_clique = self.clique_batch_norms[i](x_clique)\n",
    "                x_clique = F.relu(x_clique)\n",
    "                x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                     training=self.training)\n",
    "\n",
    "                x = x + F.relu(self.clique2atom_lins[i](scatter(\n",
    "                    x_clique[col], row, dim=0, dim_size=x.size(0),\n",
    "                    reduce='mean')))\n",
    "        # print(x.shape, \"before readout\")\n",
    "        x = self.reader(x, data.batch)\n",
    "        # print(x.shape, \"after readout\")\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        x = self.atom_lin(x)\n",
    "        # print(x.shape, \"after atom_lin\")\n",
    "        \n",
    "\n",
    "        if self.inter_message_passing:\n",
    "            tree_batch = torch.repeat_interleave(data.num_cliques)\n",
    "            x_clique = scatter(x_clique, tree_batch, dim=0, dim_size=x.size(0),\n",
    "                               reduce='mean')\n",
    "            x_clique = F.dropout(x_clique, self.dropout,\n",
    "                                 training=self.training)\n",
    "            x_clique = self.clique_lin(x_clique)\n",
    "            x = x + x_clique\n",
    "\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        # print(x.shape, \"before last\")\n",
    "        x = self.lin(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dkP00CdeukM5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import tree_decomposition\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem.rdchem import BondType\n",
    "\n",
    "bonds = [BondType.SINGLE, BondType.DOUBLE, BondType.TRIPLE, BondType.AROMATIC]\n",
    "\n",
    "\n",
    "def mol_from_data(data):\n",
    "    mol = Chem.RWMol()\n",
    "\n",
    "    x = data.x if data.x.dim() == 1 else data.x[:, 0]\n",
    "    for z in x.tolist():\n",
    "        mol.AddAtom(Chem.Atom(z))\n",
    "\n",
    "    row, col = data.edge_index\n",
    "    mask = row < col\n",
    "    row, col = row[mask].tolist(), col[mask].tolist()\n",
    "\n",
    "    bond_type = data.edge_attr\n",
    "    bond_type = bond_type if bond_type.dim() == 1 else bond_type[:, 0]\n",
    "    bond_type = bond_type[mask].tolist()\n",
    "\n",
    "    for i, j, bond in zip(row, col, bond_type):\n",
    "        assert bond >= 1 and bond <= 4\n",
    "        mol.AddBond(i, j, bonds[bond - 1])\n",
    "\n",
    "    return mol.GetMol()\n",
    "\n",
    "\n",
    "class JunctionTreeData(Data):\n",
    "    def __inc__(self, key, item):\n",
    "        if key == 'tree_edge_index':\n",
    "            return self.x_clique.size(0)\n",
    "        elif key == 'atom2clique_index':\n",
    "            return torch.tensor([[self.x.size(0)], [self.x_clique.size(0)]])\n",
    "        else:\n",
    "            return super(JunctionTreeData, self).__inc__(key, item)\n",
    "\n",
    "\n",
    "class JunctionTree(object):\n",
    "    def __call__(self, data):\n",
    "        mol = mol_from_data(data)\n",
    "        out = tree_decomposition(mol, return_vocab=True)\n",
    "        tree_edge_index, atom2clique_index, num_cliques, x_clique = out\n",
    "\n",
    "        data = JunctionTreeData(**{k: v for k, v in data})\n",
    "\n",
    "        data.tree_edge_index = tree_edge_index\n",
    "        data.atom2clique_index = atom2clique_index\n",
    "        data.num_cliques = num_cliques\n",
    "        data.x_clique = x_clique\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-BnRJKkukQ8"
   },
   "outputs": [],
   "source": [
    "# edit the function causing the error: add argument chem=None + modify function code: \n",
    "# Chem=chem if chem is not None else Chem\n",
    "\n",
    "# tree_decomposition(Chem.MolFromSmiles(\"cicccc1c\"), return_vocab=True)\n",
    "# once modified and saved, restart the environmnet, comment this cell and run the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "l-eGz7HtCLCr",
    "outputId": "a5812a61-fea7-4301-dd16-9d75ea62035b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.transforms import Compose\n",
    "\n",
    "class Argparse_emulate():\n",
    "    def __init__(self, device=0, hidden_channels=256, num_layers=2, dropout=0.5,\n",
    "               epochs=100, no_inter_message_passing=\"store_true\"):\n",
    "        self.device = device\n",
    "        self.hidden_channels = hidden_channels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.epochs = epochs\n",
    "        self.no_inter_message_passing = no_inter_message_passing\n",
    "        return\n",
    "\n",
    "args = Argparse_emulate()\n",
    "# parser.add_argument('--device', type=int, default=0)\n",
    "# parser.add_argument('--hidden_channels', type=int, default=256)\n",
    "# parser.add_argument('--num_layers', type=int, default=2)\n",
    "# parser.add_argument('--dropout', type=float, default=0.5)\n",
    "# parser.add_argument('--epochs', type=int, default=100)\n",
    "# parser.add_argument('--no_inter_message_passing', action='store_true')\n",
    "# args = parser.parse_args()\n",
    "# print(args)\n",
    "\n",
    "\n",
    "class OGBTransform(object):\n",
    "    # OGB saves atom and bond types zero-index based. We need to revert that.\n",
    "    def __call__(self, data):\n",
    "        data.x[:, 0] += 1\n",
    "        data.edge_attr[:, 0] += 1\n",
    "        return data\n",
    "\n",
    "\n",
    "transform = Compose([OGBTransform(), JunctionTree()])\n",
    "\n",
    "name = 'ogbg-molhiv'\n",
    "dataset = PygGraphPropPredDataset(name, 'data', pre_transform=transform)\n",
    "\n",
    "# correct splits\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_dataset = dataset[split_idx['train']]\n",
    "val_dataset = dataset[split_idx['valid']]\n",
    "test_dataset = dataset[split_idx['test']]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, 128, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, 128, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, 128, shuffle=False)\n",
    "\n",
    "\n",
    "device = f'cuda:{args.device}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 0:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "244it [06:20,  1.60s/it]"
     ]
    }
   ],
   "source": [
    "def train(epoch, vals=False):\n",
    "    values = []\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0\n",
    "    for i,data in tqdm(enumerate(train_loader)):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        mask = ~torch.isnan(data.y)\n",
    "        out = model(data)[mask]\n",
    "        y = data.y.to(torch.float)[mask]\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(out, y)\n",
    "        loss.backward()\n",
    "        total_loss += loss.item() * data.num_graphs\n",
    "        optimizer.step()\n",
    "        # record beta and p values\n",
    "        if vals:\n",
    "            # display computational graph\n",
    "            # global g\n",
    "            #g = make_dot(out)\n",
    "            # \"a\"+9\n",
    "            pass\n",
    "\n",
    "    return total_loss / len(train_loader.dataset), values\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    y_preds, y_trues = [], []\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        y_preds.append(model(data))\n",
    "        y_trues.append(data.y)\n",
    "\n",
    "    y_pred = torch.cat(y_preds, dim=0).cpu().numpy()\n",
    "    y_true = torch.cat(y_trues, dim=0).cpu().numpy()\n",
    "\n",
    "    rocauc_list = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        # AUC is only defined when there is at least one positive data.\n",
    "        if np.sum(y_true[:, i] == 1) > 0 and np.sum(y_true[:, i] == 0) > 0:\n",
    "            # ignore nan values\n",
    "            is_labeled = y_true[:, i] == y_true[:, i]\n",
    "            rocauc_list.append(\n",
    "                roc_auc_score(y_true[is_labeled, i], y_pred[is_labeled, i]))\n",
    "\n",
    "    return {\"rocauc\": sum(rocauc_list) / len(rocauc_list)}\n",
    "\n",
    "\n",
    "values     = []\n",
    "test_perfs = []\n",
    "for run in range(10):\n",
    "    print()\n",
    "    print(f'Run {run}:')\n",
    "    print()\n",
    "    model = Net(hidden_channels=args.hidden_channels,\n",
    "            out_channels=dataset.num_tasks, num_layers=args.num_layers,\n",
    "            dropout = args.dropout if run<10 else 0.6, # edited to increase dropout\n",
    "            inter_message_passing=not args.no_inter_message_passing).to(device)\n",
    "\n",
    "    model.reset_parameters()\n",
    "    optimizer = Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    best_val_perf = test_perf = 0\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        loss, epoch_values = train(epoch, vals=True)\n",
    "        train_perf = test(train_loader)\n",
    "        val_perf = test(val_loader)\n",
    "\n",
    "        if val_perf[\"rocauc\"] > best_val_perf:\n",
    "            best_val_perf = val_perf[\"rocauc\"]\n",
    "            test_perf = test(test_loader)\n",
    "\n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, '\n",
    "              f'Train: {train_perf[\"rocauc\"]:.4f}, Val: {val_perf[\"rocauc\"]:.4f}, '\n",
    "              f'Test: {test_perf[\"rocauc\"]:.4f}')\n",
    "        if epoch % 10 == 0:\n",
    "            print(\"Recorded values:\", epoch_values)\n",
    "\n",
    "    test_perfs.append(test_perf[\"rocauc\"])\n",
    "    values.append(epoch_values[-1])\n",
    "\n",
    "test_perf = torch.tensor(test_perfs)\n",
    "print('===========================')\n",
    "print(f'Final Test: {test_perf.mean():.4f} ± {test_perf.std():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analize params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "p_epoch    = np.array([batch[\"p\"] for batch in epoch_values]).flatten()\n",
    "beta_epoch = np.array([batch[\"beta\"] for batch in epoch_values]).flatten() \n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.title(\"Evolution of parameters beta and p during training - Power Mean\")\n",
    "plt.plot(p_epoch, \"r-\", label=\"p\")\n",
    "plt.plot(beta_epoch, \"b-\", label=\"beta\")\n",
    "plt.ylim(0, 1.1*np.amax(np.maximum(p_epoch, beta_epoch)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copia de analysis_molhiv_power.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
